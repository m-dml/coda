"""Script to test data assimilation encoder on L96 observations.

This script loads trained data assimilation encoder and performs reconstructions on L96 observations generated from
simulations  generated using script from mdml_tools.

Example:
    # Testing a single model on a single dataset of observations

Example:
    # Testing a single model on multiple datasets of observations

Example:
    # Testing multiple models on a single dataset of observations

Example:
    # Testing multiple models on multiple datasets of observations
"""
import argparse
import os
from copy import deepcopy
from datetime import datetime

import h5py
import numpy as np
import submitit
import torch
from joblib import delayed, Parallel
from mdml_tools.utils import logging as mdml_logging
from omegaconf import OmegaConf
from tqdm import tqdm

from hpl.datamodule import L96InferenceDataset
from hpl.utils.postprocessing import (
    find_experiments_directories,
    load_data_assimilation_network,
    load_hydra_config,
    load_test_simulations,
)


def create_parser() -> argparse.ArgumentParser:
    """Creating the argument parser and adding the arguments.

    Available arguments:
        --output-dir: path to the output directory where the results will be saved
        --data-path: path to the .h5 file containing L96 simulations generated by script from mdml_tools
        --n-simulations: number of simulation to load from data file. If not provided, all simulations will be loaded
        --experiment-dir: path to the experiment directory where hydra saved results
        --experiment-file: path to yaml file containing the list of directories where hydra saved results. provide this
            instead of experiment-dir to test multiple models at once
        --ignore-failed: ignore failed experiments. default: False
        --noise-std: standard deviation of Gaussian noise added to ground truth
        --noise-std-min: minimum standard deviation of Gaussian noise added to ground truth. used for mesh testing
        --noise-std-max: maximum standard deviation of Gaussian noise added to ground truth. used for mesh testing
        --mask-fraction: fraction of data points masked per time step
        --mask-fraction-min: minimum fraction of data points masked per time step. used for mesh testing
        --mask-fraction-max: maximum fraction of data points masked per time step. used for mesh testing
        --mesh-steps: number of steps for mesh testing, i.e. number of different noise and mask fractions to test
        --mesh-test: perform mesh testing. default: False
        --ignore-edges: remove edges cases from dataset. default: False
        --device: device to use for generating the data. Can be any string that is accepted by torch.device()
        --seed: seed to use for generating the mask and noise
        --n-jobs: number of processes to run in parallel. used for testing multiple models at once or mesh testing
        --slurm-array-parallelism: number of models to perform mesh test in parallel on the cluster
        --timeout-min: timeout in minutes for each SLURM job in the array
        --slurm-partition: partition to submit SLURM jobs to
    """
    parser = argparse.ArgumentParser(
        description="Make reconstructions from pseudo-observations generated from provided simulations using trained "
        "data assimilation encoder."
    )
    parser.add_argument(
        "--output-dir", type=str, help="path to the output directory where the results will be saved", required=True
    )
    parser.add_argument("--data-path", type=str, help="path to the .h5 file containing L96 simulations", required=True)
    parser.add_argument(
        "--n-simulations",
        type=int,
        help="number of simulation to load from data file If not provided, all simulations will be loaded",
    )

    parser.add_argument("--experiment-dir", type=str, help="path to the experiment directory where hydra saved results")
    parser.add_argument(
        "--experiment-file",
        type=str,
        help="path to yaml file containing the list of directories where hydra saved results",
    )
    parser.add_argument("--ignore-failed", action="store_true", help="ignore failed experiments. default: False")

    parser.add_argument(
        "--noise-std", type=float, help="standard deviation of Gaussian noise added to ground truth", default=1.0
    )
    parser.add_argument(
        "--noise-std-min",
        type=float,
        help="minimum standard deviation of Gaussian noise added to ground truth",
        default=0.1,
    )
    parser.add_argument(
        "--noise-std-max",
        type=float,
        help="maximum standard deviation of Gaussian noise added to ground truth",
        default=3.0,
    )
    parser.add_argument(
        "--mask-fraction", type=float, help="fraction of data points masked per time step", default=0.75
    )
    parser.add_argument(
        "--mask-fraction-min", type=float, help="minimum fraction of data points masked per time step", default=0.1
    )
    parser.add_argument(
        "--mask-fraction-max", type=float, help="maximum fraction of data points masked per time step", default=0.9
    )
    parser.add_argument("--mesh-steps", type=int, help="number of steps for mesh testing", default=15)
    parser.add_argument("--mesh-test", action="store_true", help="mesh testing. default: False")
    parser.add_argument("--ignore-edges", action="store_true", help="remove edges cases from dataset")

    parser.add_argument(
        "--device",
        type=str,
        default="cpu",
        help="device to use for generating the data. Can be any string that is accepted by torch.device()",
    )
    parser.add_argument("--seed", type=int, help="seed to use for generating the mask and noise", default=101)
    parser.add_argument(
        "--n-jobs",
        type=int,
        help="number of processes to run in parallel. used for testing multiple models at once or mesh testing",
        default=1,
    )
    parser.add_argument(
        "--slurm-array-parallelism",
        type=int,
        help="number of models to perform mesh test in parallel on the cluster",
        default=1,
    )
    parser.add_argument(
        "--timeout-min", type=int, help="timeout in minutes for each SLURM job in the array", default=3600
    )
    parser.add_argument("--slurm-partition", type=str, help="partition to submit SLURM jobs to", default="pGPU")
    return parser


def parse_args(parser: argparse.ArgumentParser) -> argparse.Namespace:
    """Convert the arguments to a namespace and check if they are valid.

    Args:
        parser (argparse.ArgumentParser): The argument parser.

    Returns:
        (str): The parsed arguments.
    """
    mdml_logging.get_logger()
    args = parser.parse_args()

    if args.experiment_dir is None and args.experiment_file is None:
        raise ValueError("Either --experiment-dir or --experiment-file must be provided.")
    if args.experiment_dir and args.experiment_file:
        raise ValueError("Only one of --experiment-dir or --experiment-file must be provided.")
    if args.experiment_dir:
        args.experiment_file = "-"

    try:
        torch.device(args.device)
    except Exception as msg:
        raise ValueError(f"Could not parse the device {args.device}.") from msg

    return args


def is_valid_experiment(args: argparse.Namespace, directory: str):
    """Check if the provided directory contains a valid experiment. Valid experiment should contain a hydra config file
    and a model checkpoint.

    Args:
        args (argparse.Namespace): arguments passed from the command line.
        directory (str): path to the directory to check.
    """
    logger = mdml_logging.get_logger()

    has_config = True
    try:
        load_hydra_config(directory)
    except FileNotFoundError as msg:
        logger.warning(msg)
        has_config = False

    has_checkpoint = True
    try:
        load_data_assimilation_network(directory, args.device)
    except FileNotFoundError as msg:
        logger.warning(msg)
        has_checkpoint = False

    return has_config and has_checkpoint


def search_valid_experiments(args: argparse.Namespace):
    """Search for valid experiments in the provided directory.

    Args:
        args (argparse.Namespace): arguments passed from the command line.
    """
    logger = mdml_logging.get_logger()
    directories_to_scan = OmegaConf.load(args.experiment_file).directories
    directories = find_experiments_directories(directories_to_scan)
    logger.info(f"Found {len(directories)} directories containing .hydra directory.")

    valid_experiments = []
    for directory in directories:
        if is_valid_experiment(args, directory):
            valid_experiments.append(directory)

    if len(valid_experiments) < len(directories):
        if args.ignore_failed:
            logger.warning(f"Found only {len(valid_experiments)} directories containing configs and checkpoints.")
        else:
            raise ValueError("Not all experiments are valid. Use --ignore-failed flag to ignore failed experiments.")
    return valid_experiments


def load_test_data(args: argparse.Namespace) -> torch.Tensor:
    data = load_test_simulations(args.data_path, args.n_simulations)
    args.n_simulations = data.size(0)
    return data


def test_single_model(
    args: argparse.Namespace,
    directory: str = None,
    noise_std: float = None,
    mask_fraction: float = None,
    progress_bar: bool = False,
):
    """Test a single model on a single dataset of observations.

    --mask-fraction and --noise-std are overwritten if provided, otherwise the values from arguments of script.
    Results are saved in a .h5 file in the output directory.

    Args:
        args (argparse.Namespace): arguments passed from the command line.
        directory (str): if provided, the experiment directory in arguments is overwritten.
        noise_std (float): if provided, the noise standard deviation in arguments is overwritten.
        mask_fraction (float): if provided, the mask fraction in arguments is overwritten.
        progress_bar (bool): if True, a progress bar is shown.
    """
    logger = mdml_logging.get_logger()
    arguments = deepcopy(args)
    arguments.experiment_dir = directory if directory else arguments.experiment_dir
    arguments.noise_std = noise_std if noise_std else arguments.noise_std
    arguments.mask_fraction = mask_fraction if mask_fraction else arguments.mask_fraction
    output_file = os.path.join(
        arguments.output_dir,
        f"lorenz96-sigma_{round(arguments.noise_std, 2)}-mask_{round(arguments.mask_fraction, 2)}-reconstruction.h5",
    )

    config = load_hydra_config(arguments.experiment_dir)
    model = load_data_assimilation_network(arguments.experiment_dir, arguments.device)
    simulations = load_test_data(arguments)
    dataset = L96InferenceDataset(
        ground_truth_data=simulations,
        input_window_extend=config.datamodule.dataset.input_window_extend,
        mask_fraction=arguments.mask_fraction,
        additional_noise_std=arguments.noise_std,
        drop_edge_samples=arguments.ignore_edges,
    )
    dataset.to(arguments.device)

    with h5py.File(output_file, "w") as f:
        logger.info("Saving Metadata to hdf5 file.")
        # save metadata:
        for k, v in vars(arguments).items():
            f.attrs[k] = v

        f.create_dataset(
            "reconstruction",
            shape=(arguments.n_simulations, len(dataset), simulations.size(-1)),
            dtype=np.float32,
        )

        # add dimensions to dataset:
        f["reconstruction"].dims[0].label = "simulation"
        f["reconstruction"].dims[1].label = "time"
        f["reconstruction"].dims[2].label = "grid"

        with torch.no_grad():
            iterator = enumerate(dataset)
            if progress_bar:
                iterator = tqdm(iterator, total=len(dataset))
            for i, sample in iterator:
                encoded_state = model.forward(sample)
                f["reconstruction"][:, i, :] = encoded_state.squeeze().cpu().numpy()


def mesh_test_single_model(args: argparse.Namespace, directory: str):
    """Test a single model on a mesh of noise standard deviations and mask fractions.

    This function calls test_single_model for each combination of noise standard deviation and mask fraction.
    Mesh is controlled by following script arguments (see script help for details):
        --noise-std-min
        --noise-std-max
        --mask-fraction-min
        --mask-fraction-max
        --mesh-steps
    Tests on the mesh can be run in parallel. Number of parallel processes is controlled by --n_jobs.

    Args:
        args (argparse.Namespace): arguments passed from the command line.
        directory (str): if provided, the experiment directory in arguments is overwritten.
    """
    noise_std_values = torch.linspace(args.noise_std_min, args.noise_std_max, args.mesh_steps)
    mask_fraction_values = torch.linspace(args.mask_fraction_min, args.mask_fraction_max, args.mesh_steps)
    noise_std_mesh, mask_fraction_mesh = torch.meshgrid(noise_std_values, mask_fraction_values)
    settings = [(a.item(), b.item()) for a, b in zip(noise_std_mesh.flatten(), mask_fraction_mesh.flatten())]
    Parallel(n_jobs=args.n_jobs)(delayed(test_single_model)(args, directory, a, b, False) for a, b in tqdm(settings))


def test_multiple_models(args: argparse.Namespace):
    """Test multiple models on a single observations dataset.

    This function calls test_single_model for each valid hydra experiment directory found in --experiment-dir.
    Several models can be tested in parallel. Number of parallel processes is controlled by --n_jobs.

    Args:
        args (argparse.Namespace): arguments passed from the command line.
    """
    mdml_logging.get_logger()
    directories = search_valid_experiments(args)

    setup = []
    for i, directory in enumerate(directories):
        arguments = deepcopy(args)
        arguments.output_dir = os.path.join(arguments.output_dir, str(i))
        os.makedirs(arguments.output_dir, exist_ok=True)
        setup.append((arguments, directory))

    Parallel(n_jobs=args.n_jobs)(
        delayed(test_single_model)(arguments, directory, None, None, False) for arguments, directory in tqdm(setup)
    )


def mesh_test_multiple_models(args: argparse.Namespace, executor: submitit.AutoExecutor):
    """Test multiple models on a mesh of noise standard deviations and mask fractions.

    This function calls mesh_test_single_model for each valid hydra experiment directory found in --experiment-dir.
    Test of each model is submitted as a separate job to a SLURM cluster. Number of parallel jobs is controlled by
    --slurm-array-parallelism.

    Args:
        args (argparse.Namespace): arguments passed from the command line.
    """
    mdml_logging.get_logger()
    directories = search_valid_experiments(args)

    executor.update_parameters(
        slurm_array_parallelism=args.slurm_array_parallelism,
    )

    jobs = []
    with executor.batch():
        for i, directory in enumerate(directories):
            arguments = deepcopy(args)
            arguments.output_dir = os.path.join(arguments.output_dir, str(i))
            os.makedirs(arguments.output_dir, exist_ok=True)
            job = executor.submit(mesh_test_single_model, arguments, directory)
            jobs.append(job)


if __name__ == "__main__":
    parser = create_parser()
    parsed_args = parse_args(parser)

    # create output directory
    timestamp = datetime.now().strftime("%Y-%d-%m-%H-%M")
    parsed_args.output_dir = os.path.join(parsed_args.output_dir, timestamp)
    os.makedirs(parsed_args.output_dir, exist_ok=True)

    submitit_dir = os.path.join(parsed_args.output_dir, ".submitit")
    os.makedirs(submitit_dir, exist_ok=True)
    executor = submitit.AutoExecutor(folder=submitit_dir)
    executor.update_parameters(
        timeout_min=parsed_args.timeout_min,
        slurm_partition=parsed_args.slurm_partition,
    )

    if parsed_args.experiment_dir:
        if parsed_args.mesh_test:
            executor.submit(mesh_test_single_model, parsed_args.experiment_dir)
        else:
            executor.submit(test_single_model, parsed_args)
    elif parsed_args.experiment_file:
        if parsed_args.mesh_test:
            mesh_test_multiple_models(parsed_args, executor)
        else:
            executor.submit(test_multiple_models, parsed_args)
