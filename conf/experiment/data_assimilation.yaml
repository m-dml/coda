# @package _global_
defaults:
  - override /simulator: l96_base
  - override /assimilation_network: unet_2d_1d
  - override /loss: 4dvar
  - override /datamodule: l96_observations
  - override /lightning_module: data_assimilation_module
  - override /optimizer/data_assimilation: adam_base
  - override /lightning_trainer: trainer_base
  - override /lightning_logger/tensorboard: tensorboard_logger_base
  - override /lightning_callback/model_checkpoint: model_checkpoint_base
  - override /lightning_callback/early_stopping: early_stopping_base

  - _self_ # should be last in defaults list to override everything in structured config with here defined values

# hyperparameters of the method
rollout_size: 25
integration_window: 500
time_step: 0.01
window_past_size: 15
window_future_size: 15

random_seed: 111
#output_dir_base_path: "/gpfs/work/zinchenk/hpl_logs"
output_dir_base_path: "."
print_config: true
debug: false

datamodule:
  x_grid_size: 40
  time_step: ${time_step}
  n_integration_steps: ${integration_window}
  additional_noise_mean: 0.0
  additional_noise_std: 1.0
  n_masked_per_time_step: 20
  mask_fill_value: 0.0
  save_training_data_dir: "data"
  train_validation_split: 1.0
  shuffle_train: true
  shuffle_valid: false
  batch_size: 1
  drop_last_batch: false
  num_workers: 20
  pin_memory: false
  dataset:
    chunk_size: ${rollout_size}
    window_past_size: ${window_past_size}
    window_future_size: ${window_future_size}
  simulator:
    simulator_type: "one_level"
    forcing: 8.0
    method: "rk4"

loss:
  use_model_term: true
  alpha: 1.0

simulator:
  simulator_type: "one_level"
  forcing: 8
  method: "rk4"

lightning_module:
  rollout_length: ${rollout_size}
  time_step: ${time_step}

lightning_trainer:
  max_epochs: 120
  check_val_every_n_epoch: null
  val_check_interval: null
  limit_val_batches: 0
  num_sanity_val_steps: 0
  log_every_n_steps: 50
  detect_anomaly: false
  fast_dev_run: false
  devices: 1
  deterministic: false

lightning_logger:
  tensorboard:
    default_hp_metric: false
    log_graph: false
    name: null
    prefix: ""
    save_dir: "./logs/tensorboard"
    sub_dir: null
    version: null

lightning_callback:
  early_stopping:
    min_delta: 0.0
    mode: "min"
    monitor: "TotalLoss/Training"
    patience: 10
    verbose: true
  model_checkpoint:
    dirpath: "./logs/checkpoints/"
    filename: "best"
    mode: "min"
    monitor: "TotalLoss/Training"
    save_last: true
    save_top_k: 10
    verbose: false
    save_on_train_epoch_end: True

hydra:
  job:
    chdir: true
  sweep:
    dir: ${output_dir_base_path}/data_assimilation/multirun/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ${hydra.job.num}
  run:
    dir: ${output_dir_base_path}/data_assimilation/outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}
