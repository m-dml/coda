# @package _global_
defaults:
  - override /model: l96_param
  - override /assimilation_network: unet_2d_1d
  - override /loss: 4dvar
  - override /datamodule: l96_observations
  - override /lightning_module: parameter_tuning_module
  - override /optimizer/data_assimilation: adam_base
  - override /optimizer/parametrization: adam_base
  - override /lightning_trainer: trainer_base
  - override /lightning_logger/tensorboard: tensorboard_logger_base
  - override /lightning_callback/checkpointing: checkpoint_callback_base
  - override /lightning_callback/early_stopping: early_stopping_callback_base

  - _self_ # should be last in defaults list to override everything in structured config with here defined values

random_seed: 101
output_dir_base_path: "."

loss:
  use_model_term: true
  alpha: 1.0

datamodule:
  path: "/home/vadim/Dev/hidden-process-learning/data/l96-1-58486505.pkl"
  training_split: 0.7
  shuffle_train: true
  shuffle_valid: false
  batch_size: 5
  drop_last_batch: true
  num_workers: 32
  dataset:
    chunk_size: 25
    window_past_size: 15
    window_future_size: 15

lightning_module:
  rollout_length: 25
  time_step: 0.01

lightning_trainer:
  max_epochs: 100
  devices: 1
  detect_anomaly: true

lightning_logger:
  tensorboard:
    default_hp_metric: false # important to be False, when we want to use own hp_metrics
    save_dir: "./logs/tensorboard"
    name: null

lightning_callback:
  early_stopping:
    patience: 10
    monitor: "TotalLoss/Validation"
    mode: "min"
  checkpointing:
    monitor: "TotalLoss/Validation"
    save_top_k: 10
